
@misc{noauthor_200409666_nodate,
	title = {[2004.09666] {Data} {Efficient} and {Weakly} {Supervised} {Computational} {Pathology} on {Whole} {Slide} {Images}},
	url = {https://arxiv.org/abs/2004.09666},
	urldate = {2020-11-25},
	keywords = {toread, 657a, attention, classification, clustering, deep learning, medical imaging, weak learners},
	annote = {This could be shown as an example of attention mechanisms and many other methods all combined together. later in the course.
 },
}

@article{amiranashvili_scaling_2020,
	title = {Scaling {Imitation} {Learning} in {Minecraft}},
	url = {http://arxiv.org/abs/2007.02701},
	abstract = {Imitation learning is a powerful family of techniques for learning sensorimotor coordination in immersive environments. We apply imitation learning to attain state-of-the-art performance on hard exploration problems in the Minecraft environment. We report experiments that highlight the influence of network architecture, loss function, and data augmentation. An early version of our approach reached second place in the MineRL competition at NeurIPS 2019. Here we report stronger results that can be used as a starting point for future competition entries and related research. Our code is available at https://github.com/amiranas/minerl\_imitation\_learning.},
	urldate = {2020-11-25},
	journal = {arXiv:2007.02701 [cs, stat]},
	author = {Amiranashvili, Artemij and Dorka, Nicolai and Burgard, Wolfram and Koltun, Vladlen and Brox, Thomas},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.02701},
	keywords = {deep learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, imitation learning, data augmentation},
	file = {arXiv Fulltext PDF:files/39/Amiranashvili et al. - 2020 - Scaling Imitation Learning in Minecraft.pdf:application/pdf},
}

@misc{noauthor_deep_2016,
	title = {Deep learning},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Deep_learning&oldid=701771907},
	abstract = {Deep learning (deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers with complex structures, or otherwise composed of multiple non-linear transformations.
Deep learning is part of a broader family of machine learning methods based on learning representations of data. An observation (e.g., an image) can be represented in many ways such as a vector of intensity values per pixel, or in a more abstract way as a set of edges, regions of particular shape, etc. Some representations make it easier to learn tasks (e.g., face recognition or facial expression recognition) from examples. One of the promises of deep learning is replacing handcrafted features with efficient algorithms for unsupervised or semi-supervised feature learning and hierarchical feature extraction.
Research in this area attempts to make better representations and create models to learn these representations from large-scale unlabeled data. Some of the representations are inspired by advances in neuroscience and are loosely based on interpretation of information processing and communication patterns in a nervous system, such as neural coding which attempts to define a relationship between various stimuli and associated neuronal responses in the brain.
Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent neural networks have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks.
Alternatively, deep learning has been characterized as a buzzword, or a rebranding of neural networks.},
	language = {en},
	urldate = {2016-01-28},
	journal = {Wikipedia, the free encyclopedia},
	month = jan,
	year = {2016},
	note = {Page Version ID: 701771907},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {compact encoding, Convolutional Layer, Data Augmentation, deep learning, Ground Truth Segmentation, medical imaging, Training Image},
	pages = {234--241},
	file = {Springer Full Text PDF:files/45/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}

@article{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2020-12-29},
	journal = {arXiv:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = apr,
	year = {2018},
	note = {arXiv: 1803.01271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, recurrent networks},
	file = {arXiv Fulltext PDF:files/89/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf;arXiv.org Snapshot:files/90/1803.html:text/html},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2020-12-27},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:files/92/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@article{white_speech_1976,
	title = {Speech {Recognition}: {A} {Tutorial} {Overview}},
	volume = {9},
	issn = {0018-9162},
	shorttitle = {Speech {Recognition}},
	url = {https://www.computer.org/csdl/magazine/co/1976/05/01647361/13rRUB6SpUP},
	doi = {10.1109/C-M.1976.218586},
	abstract = {Research toward mechanical recognition of speech is laying the foundation for significant advances in pattern recognition and artificial intelligence. This paper explains the nature of some of these advances and provides an introduction to the state of the art of automatic speech recognition.},
	language = {English},
	number = {05},
	urldate = {2020-12-27},
	author = {White, G. M.},
	month = may,
	year = {1976},
	note = {Publisher: IEEE Computer Society},
	pages = {40--53},
	file = {Snapshot:files/94/13rRUB6SpUP.html:text/html},
}

@article{uijlings_selective_2013,
	title = {Selective {Search} for {Object} {Recognition}},
	volume = {104},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-013-0620-5},
	doi = {10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi. unitn.it/{\textasciitilde}uijlings/SelectiveSearch.html).},
	language = {en},
	number = {2},
	urldate = {2020-12-26},
	journal = {Int J Comput Vis},
	author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
	month = sep,
	year = {2013},
	keywords = {ablation},
	pages = {154--171},
	file = {Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:files/96/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:application/pdf},
}

@article{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2020-12-26},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1311.2524},
	keywords = {ablation, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	file = {arXiv Fulltext PDF:files/99/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detec.pdf:application/pdf;arXiv.org Snapshot:files/100/1311.html:text/html},
}

@article{coupe_different_2019,
	title = {Different languages, similar encoding efficiency: {Comparable} information rates across the human communicative niche},
	volume = {5},
	copyright = {Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
	issn = {2375-2548},
	shorttitle = {Different languages, similar encoding efficiency},
	url = {https://advances.sciencemag.org/content/5/9/eaaw2594},
	doi = {10.1126/sciadv.aaw2594},
	abstract = {{\textless}p{\textgreater}Language is universal, but it has few indisputably universal characteristics, with cross-linguistic variation being the norm. For example, languages differ greatly in the number of syllables they allow, resulting in large variation in the Shannon information per syllable. Nevertheless, all natural languages allow their speakers to efficiently encode and transmit information. We show here, using quantitative methods on a large cross-linguistic corpus of 17 languages, that the coupling between language-level (information per syllable) and speaker-level (speech rate) properties results in languages encoding similar information rates ({\textasciitilde}39 bits/s) despite wide differences in each property individually: Languages are more similar in information rates than in Shannon information or speech rate. These findings highlight the intimate feedback loops between languages’ structural properties and their speakers’ neurocognition and biology under communicative pressures. Thus, language is the product of a multiscale communicative niche construction process at the intersection of biology, environment, and culture.{\textless}/p{\textgreater}},
	language = {en},
	number = {9},
	urldate = {2020-12-16},
	journal = {Science Advances},
	author = {Coupé, Christophe and Oh, Yoon Mi and Dediu, Dan and Pellegrino, François},
	month = sep,
	year = {2019},
	keywords = {encodings, language, nlp},
	pages = {eaaw2594},
	file = {Full Text PDF:files/102/Coupé et al. - 2019 - Different languages, similar encoding efficiency .pdf:application/pdf},
}

@article{fawcett_analysing_2016,
	title = {Analysing differences between algorithm configurations through ablation},
	volume = {22},
	issn = {1572-9397},
	url = {https://doi.org/10.1007/s10732-014-9275-9},
	doi = {10.1007/s10732-014-9275-9},
	abstract = {Developers of high-performance algorithms for hard computational problems increasingly take advantage of automated parameter tuning and algorithm configuration tools, and consequently often create solvers with many parameters and vast configuration spaces. However, there has been very little work to help these algorithm developers answer questions about the high-quality configurations produced by these tools, specifically about which parameter changes contribute most to improved performance. In this work, we present an automated technique for answering such questions by performing ablation analysis between two algorithm configurations. We perform an extensive empirical analysis of our technique on five scenarios from propositional satisfiability, mixed-integer programming and AI planning, and show that in all of these scenarios more than 95 \% of the performance gains between default configurations and optimised configurations obtained from automated configuration tools can be explained by modifying the values of a small number of parameters (1–4 in the scenarios we studied). We also investigate the use of our ablation analysis procedure for producing configurations that generalise well to previously-unseen problem domains, as well as for analysing the structure of the algorithm parameter response surface near and between high-performance configurations.},
	language = {en},
	number = {4},
	urldate = {2020-12-26},
	journal = {J Heuristics},
	author = {Fawcett, Chris and Hoos, Holger H.},
	month = aug,
	year = {2016},
	keywords = {ablation},
	pages = {431--458},
}

@misc{noauthor_neurips_nodate,
	title = {{NeurIPS} 2020 : {Unsupervised} {Data} {Augmentation} for {Consistency} {Training}},
	url = {https://nips.cc/virtual/2020/public/poster_44feb0096faa8326192570788b38c1d1.html},
	urldate = {2020-12-17},
	keywords = {medical imaging, data augmentation, nlp},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	keywords = {attention, nlp},
	pages = {12},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:files/107/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@article{tan_fast_nodate,
	title = {Fast {Anomaly} {Detection} for {Streaming} {Data}},
	abstract = {This paper introduces Streaming Half-Space-Trees (HS-Trees), a fast one-class anomaly detector for evolving data streams. It requires only normal data for training and works well when anomalous data are rare. The model features an ensemble of random HS-Trees, and the tree structure is constructed without any data. This makes the method highly efﬁcient because it requires no model restructuring when adapting to evolving data streams. Our analysis shows that Streaming HS-Trees has constant amortised time complexity and constant memory requirement. When compared with a state-of-theart method, our method performs favourably in terms of detection accuracy and runtime performance. Our experimental results also show that the detection performance of Streaming HS-Trees is not sensitive to its parameter settings.},
	language = {en},
	author = {Tan, Swee Chuan and Ting, Kai Ming and Liu, Tony Fei},
	keywords = {anomaly detection},
	pages = {6},
	file = {Tan et al. - Fast Anomaly Detection for Streaming Data.pdf:files/109/Tan et al. - Fast Anomaly Detection for Streaming Data.pdf:application/pdf},
}

@article{chandola_anomaly_2009,
	title = {Anomaly detection: {A} survey},
	volume = {41},
	number = {3},
	journal = {ACM computing surveys (CSUR)},
	author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
	year = {2009},
	note = {Publisher: ACM},
	pages = {15},
}

@inproceedings{liu_isolation_2008,
	title = {Isolation forest},
	booktitle = {2008 {Eighth} {IEEE} {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
	year = {2008},
	pages = {413--422},
}

@article{liu_isolation-based_2012,
	title = {Isolation-based anomaly detection},
	volume = {6},
	number = {1},
	journal = {ACM Transactions on Knowledge Discovery from Data (TKDD)},
	author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
	year = {2012},
	note = {Publisher: Acm},
	pages = {3},
}

@inproceedings{lakshminarayanan_mondrian_2014,
	title = {Mondrian forests: {Efficient} online random forests},
	booktitle = {{NeurIPS} conference},
	author = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
	year = {2014},
	pages = {3140--3148},
}

@inproceedings{lakshminarayanan_mondrian_2016,
	title = {Mondrian forests for large-scale regression when uncertainty matters},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
	year = {2016},
	pages = {1478--1487},
}

@inproceedings{roy_mondrian_2008,
	title = {The {Mondrian} {Process}},
	booktitle = {{NeurIPS} conference},
	author = {Roy, Daniel M and Teh, Yee Whye},
	year = {2008},
	pages = {1377--1384},
}

@inproceedings{breunig_lof_2000,
	title = {{LOF}: identifying density-based local outliers},
	volume = {29},
	booktitle = {{ACM} sigmod record},
	publisher = {ACM},
	author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, Jörg},
	year = {2000},
	note = {Issue: 2},
	pages = {93--104},
}

@inproceedings{pokrajac_incremental_2007,
	title = {Incremental local outlier detection for data streams},
	booktitle = {2007 {IEEE} symposium on {CIDM}},
	publisher = {IEEE},
	author = {Pokrajac, Dragoljub and Lazarevic, Aleksandar and Latecki, Longin Jan},
	year = {2007},
	pages = {504--515},
}

@article{pimentel_review_2014,
	title = {A review of novelty detection},
	volume = {99},
	journal = {Signal Processing},
	author = {Pimentel, Marco AF and Clifton, David A and Clifton, Lei and Tarassenko, Lionel},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {215--249},
}

@inproceedings{scholkopf_support_2000,
	title = {Support vector method for novelty detection},
	booktitle = {{NeurIPS} conference},
	author = {Schölkopf, Bernhard and Williamson, Robert C and Smola, Alex J and Shawe-Taylor, John and Platt, John C},
	year = {2000},
	pages = {582--588},
}

@article{rousseeuw_fast_1999,
	title = {A fast algorithm for the minimum covariance determinant estimator},
	volume = {41},
	number = {3},
	journal = {Technometrics},
	author = {Rousseeuw, Peter J and Driessen, Katrien Van},
	year = {1999},
	note = {Publisher: Taylor \& Francis Group},
	pages = {212--223},
}

@book{rayana_outlier_2019,
	title = {Outlier {Detection} {Data} {Sets}},
	url = {http://odds.cs.stonybrook.edu/},
	author = {Rayana, Shebuti},
	year = {2019},
}

@book{canadian_institute_for_cybersecurity_intrusion_2017,
	title = {Intrusion {Detection} {Evaluation} {Dataset} ({CICIDS2017})},
	url = {https://www.unb.ca/cic/datasets/ids-2017.html},
	author = {{Canadian Institute for Cybersecurity}},
	year = {2017},
}

@inproceedings{sharafaldin_toward_2018,
	title = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}.},
	booktitle = {{ICISSP}},
	author = {Sharafaldin, Iman and Lashkari, Arash Habibi and Ghorbani, Ali A},
	year = {2018},
	pages = {108--116},
}

@incollection{yeh_anomaly_2009,
	title = {Anomaly detection via over-sampling principal component analysis},
	booktitle = {New {Advances} in {Intelligent} {Decision} {Technologies}},
	publisher = {Springer},
	author = {Yeh, Yi-Ren and Lee, Zheng-Yi and Lee, Yuh-Jye},
	year = {2009},
	pages = {449--458},
}

@article{lee_anomaly_2013,
	title = {Anomaly detection via online oversampling principal component analysis},
	volume = {25},
	number = {7},
	journal = {IEEE transactions on knowledge and data engineering},
	author = {Lee, Yuh-Jye and Yeh, Yi-Ren and Wang, Yu-Chiang Frank},
	year = {2013},
	note = {Publisher: IEEE},
	pages = {1460--1470},
}

@inproceedings{ahmed_online_2009,
	title = {Online anomaly detection using {KDE}},
	booktitle = {2009 {IEEE} conference on global telecommunications},
	publisher = {IEEE},
	author = {Ahmed, Tarem},
	year = {2009},
	pages = {1--8},
}

@inproceedings{saffari_-line_2009,
	title = {On-line random forests},
	booktitle = {2009 {IEEE} {ICCV} workshops},
	publisher = {IEEE},
	author = {Saffari, Amir and Leistner, Christian and Santner, Jakob and Godec, Martin and Bischof, Horst},
	year = {2009},
	pages = {1393--1400},
}

@inproceedings{domingos_mining_2000,
	title = {Mining high-speed data streams},
	volume = {2},
	booktitle = {{KDD}},
	author = {Domingos, Pedro and Hulten, Geoff},
	year = {2000},
	pages = {71--80},
}

@inproceedings{abdulsalam_streaming_2007,
	title = {Streaming random forests},
	booktitle = {11th {IDEAS} 2007},
	publisher = {IEEE},
	author = {Abdulsalam, Hanady and Skillicorn, David B and Martin, Patrick},
	year = {2007},
	pages = {225--232},
}

@article{ghojogh_theory_2019,
	title = {The {Theory} {Behind} {Overfitting}, {Cross} {Validation}, {Regularization}, {Bagging}, and {Boosting}: {Tutorial}},
	journal = {arXiv preprint arXiv:1905.12787},
	author = {Ghojogh, Benyamin and Crowley, Mark},
	year = {2019},
}

@article{geurts_extremely_2006,
	title = {Extremely randomized trees},
	volume = {63},
	number = {1},
	journal = {Machine learning},
	author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
	year = {2006},
	note = {Publisher: Springer},
	pages = {3--42},
}

@article{criminisi_decision_2012,
	title = {Decision forests: {A} unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning},
	volume = {7},
	number = {2–3},
	journal = {Foundations and Trends® in Computer Graphics and Vision},
	author = {Criminisi, Antonio and Shotton, Jamie and Konukoglu, Ender},
	year = {2012},
	note = {Publisher: Now Publishers, Inc.},
	pages = {81--227},
}

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	number = {1},
	journal = {Machine learning},
	author = {Breiman, Leo},
	year = {2001},
	note = {Publisher: Springer},
	pages = {5--32},
}

@book{barnett_elements_1974,
	title = {Elements of sampling theory},
	publisher = {English Universities Press, London},
	author = {Barnett, Vick},
	year = {1974},
}

@inproceedings{fan_binary_2019,
	title = {Binary {Space} {Partitioning} {Forests}},
	volume = {89},
	booktitle = {22nd {AISTATS} conference},
	author = {Fan, Xuhui and Li, Bin and Sisson, Scott Anthony},
	year = {2019},
	pages = {1--10},
}

@inproceedings{fan_binary_2018,
	title = {The binary space partitioning-tree process},
	volume = {84},
	booktitle = {21st {AISTATS} conference},
	author = {Fan, Xuhui and Li, Bin and Sisson, Scott Anthony},
	year = {2018},
	pages = {1--9},
}

@book{preiss_data_2000,
	title = {Data {Structures} and {Algorithms} with {Object}-{Oriented} {Design} {Patterns} in {Java}},
	publisher = {John Wiley \& Sons Incorporated},
	author = {Preiss, Bruno R.},
	year = {2000},
}

@phdthesis{louppe_understanding_2014,
	type = {{PhD} {Thesis}},
	title = {Understanding random forests: {From} theory to practice},
	school = {University of Liège, Faculty of Applied Sciences},
	author = {Louppe, Gilles},
	year = {2014},
}

@book{murphy_machine_2012,
	title = {Machine {Learning}: {A} {Probabilistic} {Perspective}},
	url = {http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020},
	publisher = {MIT Press},
	author = {Murphy, Kevin},
	year = {2012},
	keywords = {text},
}

@book{duda_pattern_2000,
	title = {Pattern {Classification}},
	publisher = {Wiley Interscience},
	author = {Duda, R O and Hart, P E and Stork, D G},
	year = {2000},
	keywords = {text},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {text},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	number = {7553},
	journal = {nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {seminal},
	pages = {436--444},
}
