%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Mark Crowley at 2021-04-11 16:08:38 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{amiranashvili_scaling_2020,
	abstract = {Imitation learning is a powerful family of techniques for learning sensorimotor coordination in immersive environments. We apply imitation learning to attain state-of-the-art performance on hard exploration problems in the Minecraft environment. We report experiments that highlight the influence of network architecture, loss function, and data augmentation. An early version of our approach reached second place in the MineRL competition at NeurIPS 2019. Here we report stronger results that can be used as a starting point for future competition entries and related research. Our code is available at https://github.com/amiranas/minerl\_imitation\_learning.},
	author = {Amiranashvili, Artemij and Dorka, Nicolai and Burgard, Wolfram and Koltun, Vladlen and Brox, Thomas},
	file = {arXiv Fulltext PDF:files/39/Amiranashvili et al. - 2020 - Scaling Imitation Learning in Minecraft.pdf:application/pdf},
	journal = {arXiv:2007.02701 [cs, stat]},
	keywords = {deep learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, imitation learning, data augmentation},
	month = jul,
	note = {arXiv: 2007.02701},
	title = {Scaling {Imitation} {Learning} in {Minecraft}},
	url = {http://arxiv.org/abs/2007.02701},
	urldate = {2020-11-25},
	year = {2020},
	Bdsk-Url-1 = {http://arxiv.org/abs/2007.02701}}

@misc{noauthor_deep_2016,
	abstract = {Deep learning (deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers with complex structures, or otherwise composed of multiple non-linear transformations.
Deep learning is part of a broader family of machine learning methods based on learning representations of data. An observation (e.g., an image) can be represented in many ways such as a vector of intensity values per pixel, or in a more abstract way as a set of edges, regions of particular shape, etc. Some representations make it easier to learn tasks (e.g., face recognition or facial expression recognition) from examples. One of the promises of deep learning is replacing handcrafted features with efficient algorithms for unsupervised or semi-supervised feature learning and hierarchical feature extraction.
Research in this area attempts to make better representations and create models to learn these representations from large-scale unlabeled data. Some of the representations are inspired by advances in neuroscience and are loosely based on interpretation of information processing and communication patterns in a nervous system, such as neural coding which attempts to define a relationship between various stimuli and associated neuronal responses in the brain.
Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent neural networks have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks.
Alternatively, deep learning has been characterized as a buzzword, or a rebranding of neural networks.},
	copyright = {Creative Commons Attribution-ShareAlike License},
	journal = {Wikipedia, the free encyclopedia},
	language = {en},
	month = jan,
	note = {Page Version ID: 701771907},
	title = {Deep learning},
	url = {https://en.wikipedia.org/w/index.php?title=Deep_learning&oldid=701771907},
	urldate = {2016-01-28},
	year = {2016},
	Bdsk-Url-1 = {https://en.wikipedia.org/w/index.php?title=Deep_learning&oldid=701771907}}

@inproceedings{ronneberger_u-net_2015,
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	address = {Cham},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} -- {MICCAI} 2015},
	doi = {10.1007/978-3-319-24574-4_28},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	file = {Springer Full Text PDF:files/45/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
	isbn = {978-3-319-24574-4},
	keywords = {deep learning, medical imaging, Convolutional Layer, Data Augmentation, Ground Truth Segmentation, Training Image, compact encoding},
	language = {en},
	pages = {234--241},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {U-{Net}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-319-24574-4_28}}

@article{bai_empirical_2018,
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	file = {arXiv Fulltext PDF:files/89/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf;arXiv.org Snapshot:files/90/1803.html:text/html},
	journal = {arXiv:1803.01271 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, recurrent networks, gradient optimizers},
	month = apr,
	note = {arXiv: 1803.01271},
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	urldate = {2020-12-29},
	year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1803.01271}}

@inproceedings{pennington_glove_2014,
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	address = {Doha, Qatar},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	doi = {10.3115/v1/D14-1162},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:files/92/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
	language = {en},
	pages = {1532--1543},
	publisher = {Association for Computational Linguistics},
	shorttitle = {Glove},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	url = {http://aclweb.org/anthology/D14-1162},
	urldate = {2020-12-27},
	year = {2014},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D14-1162},
	Bdsk-Url-2 = {https://doi.org/10.3115/v1/D14-1162}}

@article{white_speech_1976,
	abstract = {Research toward mechanical recognition of speech is laying the foundation for significant advances in pattern recognition and artificial intelligence. This paper explains the nature of some of these advances and provides an introduction to the state of the art of automatic speech recognition.},
	author = {White, G. M.},
	doi = {10.1109/C-M.1976.218586},
	file = {Snapshot:files/94/13rRUB6SpUP.html:text/html},
	issn = {0018-9162},
	language = {English},
	month = may,
	note = {Publisher: IEEE Computer Society},
	number = {05},
	pages = {40--53},
	shorttitle = {Speech {Recognition}},
	title = {Speech {Recognition}: {A} {Tutorial} {Overview}},
	url = {https://www.computer.org/csdl/magazine/co/1976/05/01647361/13rRUB6SpUP},
	urldate = {2020-12-27},
	volume = {9},
	year = {1976},
	Bdsk-Url-1 = {https://www.computer.org/csdl/magazine/co/1976/05/01647361/13rRUB6SpUP},
	Bdsk-Url-2 = {https://doi.org/10.1109/C-M.1976.218586}}

@article{uijlings_selective_2013,
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi. unitn.it/{\textasciitilde}uijlings/SelectiveSearch.html).},
	author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
	doi = {10.1007/s11263-013-0620-5},
	file = {Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:files/96/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:application/pdf},
	issn = {0920-5691, 1573-1405},
	journal = {International Journal of Computer Vision},
	keywords = {ablation},
	language = {en},
	month = sep,
	number = {2},
	pages = {154--171},
	title = {Selective {Search} for {Object} {Recognition}},
	url = {http://link.springer.com/10.1007/s11263-013-0620-5},
	urldate = {2020-12-26},
	volume = {104},
	year = {2013},
	Bdsk-Url-1 = {http://link.springer.com/10.1007/s11263-013-0620-5},
	Bdsk-Url-2 = {https://doi.org/10.1007/s11263-013-0620-5}}

@article{girshick_rich_2014,
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	file = {arXiv Fulltext PDF:files/99/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detec.pdf:application/pdf;arXiv.org Snapshot:files/100/1311.html:text/html},
	journal = {arXiv:1311.2524 [cs]},
	keywords = {ablation, Computer Science - Computer Vision and Pattern Recognition},
	month = oct,
	note = {arXiv: 1311.2524},
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	urldate = {2020-12-26},
	year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1311.2524}}

@article{coupe_different_2019,
	abstract = {{\textless}p{\textgreater}Language is universal, but it has few indisputably universal characteristics, with cross-linguistic variation being the norm. For example, languages differ greatly in the number of syllables they allow, resulting in large variation in the Shannon information per syllable. Nevertheless, all natural languages allow their speakers to efficiently encode and transmit information. We show here, using quantitative methods on a large cross-linguistic corpus of 17 languages, that the coupling between language-level (information per syllable) and speaker-level (speech rate) properties results in languages encoding similar information rates ({\textasciitilde}39 bits/s) despite wide differences in each property individually: Languages are more similar in information rates than in Shannon information or speech rate. These findings highlight the intimate feedback loops between languages' structural properties and their speakers' neurocognition and biology under communicative pressures. Thus, language is the product of a multiscale communicative niche construction process at the intersection of biology, environment, and culture.{\textless}/p{\textgreater}},
	author = {Coup{\'e}, Christophe and Oh, Yoon Mi and Dediu, Dan and Pellegrino, Fran{\c c}ois},
	copyright = {Copyright {\copyright} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
	doi = {10.1126/sciadv.aaw2594},
	file = {Full Text PDF:files/102/Coup{\'e} et al. - 2019 - Different languages, similar encoding efficiency .pdf:application/pdf},
	issn = {2375-2548},
	journal = {Science Advances},
	keywords = {encodings, language, nlp},
	language = {en},
	month = sep,
	number = {9},
	pages = {eaaw2594},
	shorttitle = {Different languages, similar encoding efficiency},
	title = {Different languages, similar encoding efficiency: {Comparable} information rates across the human communicative niche},
	url = {https://advances.sciencemag.org/content/5/9/eaaw2594},
	urldate = {2020-12-16},
	volume = {5},
	year = {2019},
	Bdsk-Url-1 = {https://advances.sciencemag.org/content/5/9/eaaw2594},
	Bdsk-Url-2 = {https://doi.org/10.1126/sciadv.aaw2594}}

@article{fawcett_analysing_2016,
	abstract = {Developers of high-performance algorithms for hard computational problems increasingly take advantage of automated parameter tuning and algorithm configuration tools, and consequently often create solvers with many parameters and vast configuration spaces. However, there has been very little work to help these algorithm developers answer questions about the high-quality configurations produced by these tools, specifically about which parameter changes contribute most to improved performance. In this work, we present an automated technique for answering such questions by performing ablation analysis between two algorithm configurations. We perform an extensive empirical analysis of our technique on five scenarios from propositional satisfiability, mixed-integer programming and AI planning, and show that in all of these scenarios more than 95 \% of the performance gains between default configurations and optimised configurations obtained from automated configuration tools can be explained by modifying the values of a small number of parameters (1--4 in the scenarios we studied). We also investigate the use of our ablation analysis procedure for producing configurations that generalise well to previously-unseen problem domains, as well as for analysing the structure of the algorithm parameter response surface near and between high-performance configurations.},
	author = {Fawcett, Chris and Hoos, Holger H.},
	doi = {10.1007/s10732-014-9275-9},
	issn = {1572-9397},
	journal = {Journal of Heuristics},
	keywords = {ablation},
	language = {en},
	month = aug,
	number = {4},
	pages = {431--458},
	title = {Analysing differences between algorithm configurations through ablation},
	url = {https://doi.org/10.1007/s10732-014-9275-9},
	urldate = {2020-12-26},
	volume = {22},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10732-014-9275-9}}

@misc{noauthor_neurips_nodate,
	keywords = {medical imaging, data augmentation, nlp},
	title = {{NeurIPS} 2020 : {Unsupervised} {Data} {Augmentation} for {Consistency} {Training}},
	url = {https://nips.cc/virtual/2020/public/poster_44feb0096faa8326192570788b38c1d1.html},
	urldate = {2020-12-17},
	Bdsk-Url-1 = {https://nips.cc/virtual/2020/public/poster_44feb0096faa8326192570788b38c1d1.html}}

@article{radford_improving_nodate,
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:files/107/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
	keywords = {attention, nlp},
	language = {en},
	pages = {12},
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}}}

@article{tan_fast_nodate,
	abstract = {This paper introduces Streaming Half-Space-Trees (HS-Trees), a fast one-class anomaly detector for evolving data streams. It requires only normal data for training and works well when anomalous data are rare. The model features an ensemble of random HS-Trees, and the tree structure is constructed without any data. This makes the method highly efficient because it requires no model restructuring when adapting to evolving data streams. Our analysis shows that Streaming HS-Trees has constant amortised time complexity and constant memory requirement. When compared with a state-of-theart method, our method performs favourably in terms of detection accuracy and runtime performance. Our experimental results also show that the detection performance of Streaming HS-Trees is not sensitive to its parameter settings.},
	author = {Tan, Swee Chuan and Ting, Kai Ming and Liu, Tony Fei},
	file = {Tan et al. - Fast Anomaly Detection for Streaming Data.pdf:files/109/Tan et al. - Fast Anomaly Detection for Streaming Data.pdf:application/pdf},
	keywords = {anomaly detection},
	language = {en},
	pages = {6},
	title = {Fast {Anomaly} {Detection} for {Streaming} {Data}}}

@article{chandola_anomaly_2009,
	author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
	journal = {ACM computing surveys (CSUR)},
	keywords = {anomaly detection},
	note = {Publisher: ACM},
	number = {3},
	pages = {15},
	title = {Anomaly detection: {A} survey},
	volume = {41},
	year = {2009}}

@inproceedings{liu_isolation_2008,
	author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
	booktitle = {2008 {Eighth} {IEEE} {International} {Conference} on {Data} {Mining}},
	keywords = {anomaly detection},
	pages = {413--422},
	publisher = {IEEE},
	title = {Isolation forest},
	year = {2008}}

@article{liu_isolation-based_2012,
	author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
	journal = {ACM Transactions on Knowledge Discovery from Data (TKDD)},
	keywords = {anomaly detection},
	note = {Publisher: Acm},
	number = {1},
	pages = {3},
	title = {Isolation-based anomaly detection},
	volume = {6},
	year = {2012}}

@inproceedings{lakshminarayanan_mondrian_2014,
	author = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
	booktitle = {{NeurIPS} conference},
	keywords = {ensemble methods, streaming ensembles},
	pages = {3140--3148},
	title = {Mondrian forests: {Efficient} online random forests},
	year = {2014}}

@inproceedings{lakshminarayanan_mondrian_2016,
	author = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	keywords = {ensemble methods, streaming ensembles},
	pages = {1478--1487},
	title = {Mondrian forests for large-scale regression when uncertainty matters},
	year = {2016}}

@inproceedings{roy_mondrian_2008,
	author = {Roy, Daniel M and Teh, Yee Whye},
	booktitle = {{NeurIPS} conference},
	keywords = {ensemble methods, streaming ensembles},
	pages = {1377--1384},
	title = {The {Mondrian} {Process}},
	year = {2008}}

@inproceedings{breunig_lof_2000,
	author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
	booktitle = {{ACM} sigmod record},
	keywords = {anomaly detection},
	note = {Issue: 2},
	pages = {93--104},
	publisher = {ACM},
	title = {{LOF}: identifying density-based local outliers},
	volume = {29},
	year = {2000}}

@inproceedings{pokrajac_incremental_2007,
	author = {Pokrajac, Dragoljub and Lazarevic, Aleksandar and Latecki, Longin Jan},
	booktitle = {2007 {IEEE} symposium on {CIDM}},
	pages = {504--515},
	publisher = {IEEE},
	title = {Incremental local outlier detection for data streams},
	year = {2007}}

@article{pimentel_review_2014,
	author = {Pimentel, Marco AF and Clifton, David A and Clifton, Lei and Tarassenko, Lionel},
	journal = {Signal Processing},
	keywords = {anomaly detection},
	note = {Publisher: Elsevier},
	pages = {215--249},
	title = {A review of novelty detection},
	volume = {99},
	year = {2014}}

@inproceedings{scholkopf_support_2000,
	author = {Sch{\"o}lkopf, Bernhard and Williamson, Robert C and Smola, Alex J and Shawe-Taylor, John and Platt, John C},
	booktitle = {{NeurIPS} conference},
	keywords = {anomaly detection, support vector machines},
	pages = {582--588},
	title = {Support vector method for novelty detection},
	year = {2000}}

@article{rousseeuw_fast_1999,
	author = {Rousseeuw, Peter J and Driessen, Katrien Van},
	journal = {Technometrics},
	note = {Publisher: Taylor \& Francis Group},
	number = {3},
	pages = {212--223},
	title = {A fast algorithm for the minimum covariance determinant estimator},
	volume = {41},
	year = {1999}}

@book{rayana_outlier_2019,
	author = {Rayana, Shebuti},
	keywords = {anomaly detection},
	title = {Outlier {Detection} {Data} {Sets}},
	url = {http://odds.cs.stonybrook.edu/},
	year = {2019},
	Bdsk-Url-1 = {http://odds.cs.stonybrook.edu/}}

@book{canadian_institute_for_cybersecurity_intrusion_2017,
	author = {{Canadian Institute for Cybersecurity}},
	keywords = {anomaly detection},
	title = {Intrusion {Detection} {Evaluation} {Dataset} ({CICIDS2017})},
	url = {https://www.unb.ca/cic/datasets/ids-2017.html},
	year = {2017},
	Bdsk-Url-1 = {https://www.unb.ca/cic/datasets/ids-2017.html}}

@inproceedings{sharafaldin_toward_2018,
	author = {Sharafaldin, Iman and Lashkari, Arash Habibi and Ghorbani, Ali A},
	booktitle = {{ICISSP}},
	keywords = {anomaly detection},
	pages = {108--116},
	title = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}.},
	year = {2018}}

@incollection{yeh_anomaly_2009,
	author = {Yeh, Yi-Ren and Lee, Zheng-Yi and Lee, Yuh-Jye},
	booktitle = {New {Advances} in {Intelligent} {Decision} {Technologies}},
	keywords = {anomaly detection},
	pages = {449--458},
	publisher = {Springer},
	title = {Anomaly detection via over-sampling principal component analysis},
	year = {2009}}

@article{lee_anomaly_2013,
	author = {Lee, Yuh-Jye and Yeh, Yi-Ren and Wang, Yu-Chiang Frank},
	journal = {IEEE transactions on knowledge and data engineering},
	keywords = {anomaly detection},
	note = {Publisher: IEEE},
	number = {7},
	pages = {1460--1470},
	title = {Anomaly detection via online oversampling principal component analysis},
	volume = {25},
	year = {2013}}

@inproceedings{ahmed_online_2009,
	author = {Ahmed, Tarem},
	booktitle = {2009 {IEEE} conference on global telecommunications},
	keywords = {anomaly detection},
	pages = {1--8},
	publisher = {IEEE},
	title = {Online anomaly detection using {KDE}},
	year = {2009}}

@inproceedings{saffari_-line_2009,
	author = {Saffari, Amir and Leistner, Christian and Santner, Jakob and Godec, Martin and Bischof, Horst},
	booktitle = {2009 {IEEE} {ICCV} workshops},
	keywords = {ensemble methods, streaming ensembles},
	pages = {1393--1400},
	publisher = {IEEE},
	title = {On-line random forests},
	year = {2009}}

@inproceedings{domingos_mining_2000,
	annote = {The paper that introduced Hoeffding Trees.},
	author = {Domingos, Pedro and Hulten, Geoff},
	booktitle = {{KDD}},
	keywords = {streaming ensembles},
	pages = {71--80},
	title = {Mining high-speed data streams},
	volume = {2},
	year = {2000}}

@inproceedings{abdulsalam_streaming_2007,
	author = {Abdulsalam, Hanady and Skillicorn, David B and Martin, Patrick},
	booktitle = {11th {IDEAS} 2007},
	keywords = {ensemble methods, streaming ensembles},
	pages = {225--232},
	publisher = {IEEE},
	title = {Streaming random forests},
	year = {2007}}

@article{ghojogh_theory_2019,
	author = {Ghojogh, Benyamin and Crowley, Mark},
	journal = {arXiv preprint arXiv:1905.12787},
	keywords = {tutorial},
	title = {The {Theory} {Behind} {Overfitting}, {Cross} {Validation}, {Regularization}, {Bagging}, and {Boosting}: {Tutorial}},
	year = {2019}}

@article{geurts_extremely_2006,
	author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
	journal = {Machine learning},
	keywords = {ensemble methods},
	note = {Publisher: Springer},
	number = {1},
	pages = {3--42},
	title = {Extremely randomized trees},
	volume = {63},
	year = {2006}}

@article{criminisi_decision_2012,
	author = {Criminisi, Antonio and Shotton, Jamie and Konukoglu, Ender},
	journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
	keywords = {ensemble methods},
	note = {Publisher: Now Publishers, Inc.},
	number = {2--3},
	pages = {81--227},
	title = {Decision forests: {A} unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning},
	volume = {7},
	year = {2012}}

@article{breiman_random_2001,
	author = {Breiman, Leo},
	journal = {Machine learning},
	keywords = {ensemble methods},
	note = {Publisher: Springer},
	number = {1},
	pages = {5--32},
	title = {Random forests},
	volume = {45},
	year = {2001}}

@book{barnett_elements_1974,
	author = {Barnett, Vick},
	publisher = {English Universities Press, London},
	title = {Elements of sampling theory},
	year = {1974}}

@inproceedings{fan_binary_2019,
	author = {Fan, Xuhui and Li, Bin and Sisson, Scott Anthony},
	booktitle = {22nd {AISTATS} conference},
	keywords = {ensemble methods},
	pages = {1--10},
	title = {Binary {Space} {Partitioning} {Forests}},
	volume = {89},
	year = {2019}}

@inproceedings{fan_binary_2018,
	author = {Fan, Xuhui and Li, Bin and Sisson, Scott Anthony},
	booktitle = {21st {AISTATS} conference},
	keywords = {ensemble methods},
	pages = {1--9},
	title = {The binary space partitioning-tree process},
	volume = {84},
	year = {2018}}

@book{preiss_data_2000,
	author = {Preiss, Bruno R.},
	publisher = {John Wiley \& Sons Incorporated},
	title = {Data {Structures} and {Algorithms} with {Object}-{Oriented} {Design} {Patterns} in {Java}},
	year = {2000}}

@phdthesis{louppe_understanding_2014,
	author = {Louppe, Gilles},
	keywords = {ensemble methods},
	school = {University of Li{\`e}ge, Faculty of Applied Sciences},
	title = {Understanding random forests: {From} theory to practice},
	type = {{PhD} {Thesis}},
	year = {2014}}

@book{murphy_machine_2012,
	annote = {If there is one book to help with everything in this course, this would be it.},
	author = {Murphy, Kevin},
	keywords = {text},
	publisher = {MIT Press},
	title = {Machine {Learning}: {A} {Probabilistic} {Perspective}},
	url = {http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020},
	year = {2012},
	Bdsk-Url-1 = {http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020}}

@book{duda_pattern_2000,
	author = {Duda, R O and Hart, P E and Stork, D G},
	keywords = {text},
	publisher = {Wiley Interscience},
	title = {Pattern {Classification}},
	year = {2000}}

@book{goodfellow_deep_2016,
	annote = {The notation is sometimes hard to follow, but this book has a similar structure to the course, the first part goes quickly through the basics of data analysis and preparation, the rest goes into the definitions, tradeoffs and theory of neural networks and deep learning.
Also, freely available online on the website.},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	keywords = {text},
	publisher = {MIT Press},
	title = {Deep {Learning}},
	url = {http://www.deeplearningbook.net/book},
	year = {2016},
	Bdsk-Url-1 = {http://www.deeplearningbook.net/book}}

@article{lecun_deep_2015,
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	journal = {nature},
	keywords = {seminal},
	note = {Publisher: Nature Publishing Group},
	number = {7553},
	pages = {436--444},
	title = {Deep learning},
	volume = {521},
	year = {2015}}

@article{ruder_overview_2017,
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	annote = {Comment: Added derivations of AdaMax and Nadam},
	author = {Ruder, Sebastian},
	file = {arXiv Fulltext PDF:files/151/Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:files/152/1609.html:text/html},
	journal = {arXiv:1609.04747 [cs]},
	keywords = {Computer Science - Machine Learning},
	month = jun,
	note = {arXiv: 1609.04747},
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	urldate = {2020-12-31},
	year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1609.04747}}

@article{kingma_adam_2014,
	author = {Kingma, Diederik P and Ba, Jimmy},
	journal = {arXiv preprint arXiv:1412.6980},
	keywords = {gradient optimizers, adam optimizer},
	title = {Adam: {A} method for stochastic optimization},
	year = {2014}}

@article{ruder_overview_2017-1,
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	annote = {Comment: Added derivations of AdaMax and Nadam},
	author = {Ruder, Sebastian},
	file = {arXiv Fulltext PDF:files/156/Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:files/157/1609.html:text/html},
	journal = {arXiv:1609.04747 [cs]},
	keywords = {Computer Science - Machine Learning},
	month = jun,
	note = {arXiv: 1609.04747},
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	urldate = {2021-01-03},
	year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1609.04747}}

@misc{us_environmental_protection_agency_meteorological_2000,
	annote = {This report has been reviewed by the U.S. Environmental Protection Agency (EPA) and has been approved for publication as an EPA document. Any mention of trade names or commercial products does not constitute endorsement or recommendation for use.
 },
	author = {U.S. ENVIRONMENTAL PROTECTION AGENCY},
	file = {mmgrma_0.pdf:files/172/mmgrma_0.pdf:application/pdf},
	month = feb,
	shorttitle = {Section 6.8.1 {Substitution} {Procedures}},
	title = {Meteorological {Monitoring} {Guidance} for {Regulatory} {Modeling} {Applications}},
	url = {https://www.epa.gov/sites/production/files/2020-10/documents/mmgrma_0.pdf},
	urldate = {2021-01-12},
	year = {2000},
	Bdsk-Url-1 = {https://www.epa.gov/sites/production/files/2020-10/documents/mmgrma_0.pdf}}

@article{vaswani_attention_2017,
	annote = {The initial paper on the idea of using "attention" in deep learning to focus training updates.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
	file = {Full Text PDF:files/179/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf;Snapshot:files/180/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html:text/html},
	journal = {Advances in Neural Information Processing Systems},
	language = {en},
	pages = {5998--6008},
	title = {Attention is {All} you {Need}},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2021-01-05},
	volume = {30},
	year = {2017},
	Bdsk-Url-1 = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}}

@misc{lamba_intuitive_2019,
	abstract = {A TensorFlow Implementation of Neural Machine Translation with Attention},
	author = {Lamba, Harshall},
	file = {Snapshot:files/182/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f.html:text/html},
	journal = {Medium},
	keywords = {attention, blog},
	language = {en},
	month = may,
	title = {Intuitive {Understanding} of {Attention} {Mechanism} in {Deep} {Learning}},
	url = {https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f},
	urldate = {2021-01-05},
	year = {2019},
	Bdsk-Url-1 = {https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f}}

@misc{brownlee_how_2017,
	abstract = {Attention is a mechanism that was developed to improve the performance of the Encoder-Decoder RNN on machine translation. In this tutorial, you will discover the attention mechanism for the Encoder-Decoder model. After completing this tutorial, you will know: About the Encoder-Decoder model and attention mechanism for machine translation. How to implement the attention mechanism step-by-step. [{\ldots}]},
	author = {Brownlee, Jason},
	file = {Snapshot:files/184/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks.html:text/html},
	journal = {Machine Learning Mastery},
	keywords = {attention, blog},
	language = {en-US},
	month = oct,
	title = {How {Does} {Attention} {Work} in {Encoder}-{Decoder} {Recurrent} {Neural} {Networks}},
	url = {https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/},
	urldate = {2021-01-05},
	year = {2017},
	Bdsk-Url-1 = {https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/}}

@article{lu_data_2020,
	abstract = {The rapidly emerging field of computational pathology has the potential to enable objective diagnosis, therapeutic response prediction and identification of new morphological features of clinical relevance. However, deep learning-based computational pathology approaches either require manual annotation of gigapixel whole slide images (WSIs) in fully-supervised settings or thousands of WSIs with slide-level labels in a weakly-supervised setting. Moreover, whole slide level computational pathology methods also suffer from domain adaptation and interpretability issues. These challenges have prevented the broad adaptation of computational pathology for clinical and research purposes. Here we present CLAM - Clustering-constrained attention multiple instance learning, an easy-to-use, high-throughput, and interpretable WSI-level processing and learning method that only requires slide-level labels while being data efficient, adaptable and capable of handling multi-class subtyping problems. CLAM is a deep-learning-based weakly-supervised method that uses attention-based learning to automatically identify sub-regions of high diagnostic value in order to accurately classify the whole slide, while also utilizing instance-level clustering over the representative regions identified to constrain and refine the feature space. In three separate analyses, we demonstrate the data efficiency and adaptability of CLAM and its superior performance over standard weakly-supervised classification. We demonstrate that CLAM models are interpretable and can be used to identify well-known and new morphological features. We further show that models trained using CLAM are adaptable to independent test cohorts, cell phone microscopy images, and biopsies. CLAM is a general-purpose and adaptable method that can be used for a variety of different computational pathology tasks in both clinical and research settings.},
	annote = {This is a great example of attention mechanisms being used for the Digital Pathology domain (which has a lot of research being done in UWaterloo).
Once you've ready about attention mechanisms in general this is a good way to see it applied to something impactful.
 },
	author = {Lu, Ming Y. and Williamson, Drew F. K. and Chen, Tiffany Y. and Chen, Richard J. and Barbieri, Matteo and Mahmood, Faisal},
	file = {[2004.09666] Data Efficient and Weakly Supervised .pdf:files/185/[2004.09666] Data Efficient and Weakly Supervised .pdf:application/pdf},
	journal = {arXiv:2004.09666 [cs, eess, q-bio]},
	keywords = {toread, 657a, attention, classification, clustering, deep learning, medical imaging, weak learners, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, digital pathology, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Tissues and Organs},
	month = may,
	note = {arXiv: 2004.09666},
	title = {Data {Efficient} and {Weakly} {Supervised} {Computational} {Pathology} on {Whole} {Slide} {Images}},
	url = {http://arxiv.org/abs/2004.09666},
	urldate = {2021-01-13},
	year = {2020},
	Bdsk-Url-1 = {http://arxiv.org/abs/2004.09666}}

@article{noauthor_notitle_nodate-1,
	number = {fds}}

@misc{noauthor_probmlpyprobml_2021,
	abstract = {Python code for "Machine learning: a probabilistic perspective" (2nd edition)},
	copyright = {MIT License , MIT License},
	month = jan,
	note = {original-date: 2016-08-17T16:42:24Z},
	publisher = {Probabilistic machine learning},
	title = {probml/pyprobml},
	url = {https://github.com/probml/pyprobml},
	urldate = {2021-01-15},
	year = {2021},
	Bdsk-Url-1 = {https://github.com/probml/pyprobml}}

@article{benyamin_ghojogh_ali_ghodsi_fakhri_karray_mark_crowley_factor_2020,
	author = {{Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley}},
	title = {Factor {Analysis}, {Probabilistic} {Principal} {Component} {Analysis}, {Variational} {Inference}, and {Variational} {Autoencoder}: {Tutorial} and {Survey}},
	url = {https://arxiv.org/abs/2101.00734},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2101.00734}}

@article{benyamin_ghojogh_ali_ghodsi_fakhri_karray_mark_crowley_stochastic_2020,
	author = {{Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley}},
	title = {Stochastic {Neighbor} {Embedding} with {Gaussian} and {Student}-t {Distributions}: {Tutorial} and {Survey}},
	url = {https://arxiv.org/abs/2009.10301},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2009.10301}}

@article{benyamin_ghojogh_mark_crowley_unsupervised_2019,
	author = {{Benyamin Ghojogh, Mark Crowley}},
	title = {Unsupervised and {Supervised} {Principal} {Component} {Analysis}: {Tutorial}},
	url = {https://arxiv.org/abs/1906.03148},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1906.03148}}

@article{benyamin_ghojogh_ali_ghodsi_fakhri_karray_mark_crowley_multidimensional_nodate,
	author = {{Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley}},
	title = {Multidimensional {Scaling}, {Sammon} {Mapping}, and {Isomap}: {Tutorial} and {Survey}},
	url = {https://arxiv.org/abs/2009.08136},
	Bdsk-Url-1 = {https://arxiv.org/abs/2009.08136}}

@article{benyamin_ghojogh_ali_ghodsi_fakhri_karray_mark_crowley_locally_2020,
	author = {{Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley}},
	title = {Locally {Linear} {Embedding} and its {Variants}: {Tutorial} and {Survey}},
	url = {https://arxiv.org/abs/2011.10925},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2011.10925}}

@article{benyamin_ghojogh_fakhri_karray_mark_crowley_fisher_2019,
	author = {{Benyamin Ghojogh, Fakhri Karray, Mark Crowley}},
	title = {Fisher and {Kernel} {Fisher} {Discriminant} {Analysis}: {Tutorial}},
	url = {https://arxiv.org/abs/1906.09436},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1906.09436}}

@article{benyamin_ghojogh_mark_crowley_linear_2019,
	author = {{Benyamin Ghojogh, Mark Crowley}},
	title = {Linear and {Quadratic} {Discriminant} {Analysis}: {Tutorial}},
	url = {https://arxiv.org/abs/1906.02590},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1906.02590}}

@misc{community_unsupervised_2019,
	abstract = {In our paper, we show how the application of an unsupervised machine learning model can capture information from the materials chemistry literature in a way that also uncovers latent knowledge previously unknown to the research community.

Image Credit: Olga Kononova},
	author = {Community, Nature Portfolio Chemistry},
	file = {Snapshot:files/335/50785-unsupervised-word-embeddings-capture-latent-knowledge-from-materials-science-literature.html:text/html},
	journal = {Nature Portfolio Chemistry Community},
	language = {en},
	month = jul,
	note = {Section: Behind the paper},
	title = {Unsupervised word embeddings capture latent knowledge from materials science literature},
	url = {https://chemistrycommunity.nature.com/posts/50785-unsupervised-word-embeddings-capture-latent-knowledge-from-materials-science-literature},
	urldate = {2021-02-22},
	year = {2019},
	Bdsk-Url-1 = {https://chemistrycommunity.nature.com/posts/50785-unsupervised-word-embeddings-capture-latent-knowledge-from-materials-science-literature}}

@article{tshitoyan_unsupervised_2019,
	abstract = {The overwhelming majority of scientific knowledge is published as text, which is difficult to analyse by either traditional statistical analysis or modern machine learning methods. By contrast, the main source of machine-interpretable data for the materials research community has come from structured property databases1,2, which encompass only a small fraction of the knowledge present in the research literature. Beyond property values, publications contain valuable knowledge regarding the connections and relationships between data items as interpreted by the authors. To improve the identification and use of this knowledge, several studies have focused on the retrieval of information from scientific literature using supervised natural language processing3--10, which requires large hand-labelled datasets for training. Here we show that materials science knowledge present in the published literature can be efficiently encoded as information-dense word embeddings11--13 (vector representations of words) without human labelling or supervision. Without any explicit insertion of chemical knowledge, these embeddings capture complex materials science concepts such as the underlying structure of the periodic table and structure--property relationships in materials. Furthermore, we demonstrate that an unsupervised method can recommend materials for functional applications several years before their discovery. This suggests that latent knowledge regarding future discoveries is to a large extent embedded in past publications. Our findings highlight the possibility of extracting knowledge and relationships from the massive body of scientific literature in a collective manner, and point towards a generalized approach to the mining of scientific literature.},
	annote = {Discussed during Lecture 7A on Feb 22, 2021
This paper shows a groundbreaking demonstration of the power of vector encoding learning on scientific articles.},
	author = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A. and Ceder, Gerbrand and Jain, Anubhav},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	doi = {10.1038/s41586-019-1335-8},
	file = {Submitted Version:files/338/Tshitoyan et al. - 2019 - Unsupervised word embeddings capture latent knowle.pdf:application/pdf;Snapshot:files/339/s41586-019-1335-8.html:text/html},
	issn = {1476-4687},
	journal = {Nature},
	language = {en},
	month = jul,
	note = {Number: 7763 Publisher: Nature Publishing Group},
	number = {7763},
	pages = {95--98},
	title = {Unsupervised word embeddings capture latent knowledge from materials science literature},
	url = {https://www.nature.com/articles/s41586-019-1335-8},
	urldate = {2021-02-22},
	volume = {571},
	year = {2019},
	Bdsk-Url-1 = {https://www.nature.com/articles/s41586-019-1335-8},
	Bdsk-Url-2 = {https://doi.org/10.1038/s41586-019-1335-8}}

@inproceedings{mikolov_distributed_nodate,
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26 ({NIPS} 2013)},
	file = {Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:files/340/Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:application/pdf},
	language = {en},
	pages = {9},
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}}}

@inproceedings{scholkopf_support_2000-1,
	abstract = {Suppose you are given some dataset drawn from an underlying probabil-ity distribution P and you want to estimate a ``simple '' subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified  between  and . We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a poten-tially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data. 1},
	author = {Sch{\"o}lkopf, Bernhard and Williamson, Robert and Smolax, Alex and Shawe-Taylor, John and Platt, John},
	booktitle = {Neural {Information} {Processing} {Systems}},
	file = {Citeseer - Snapshot:files/367/summary.html:text/html;Citeseer - Full Text PDF:files/368/Williamsonx et al. - MIT Press (2000) Support Vector Method for Novelty.pdf:application/pdf},
	keywords = {one-class SVM},
	pages = {582--588},
	title = {Support {Vector} {Method} for {Novelty} {Detection}},
	year = {2000}}

@article{greenwald_correlated_nodate,
	abstract = {This paper introduces Correlated-Q (CE-Q) learning, a multiagent Q-learning algorithm based on the correlated equilibrium (CE) solution concept. CE-Q generalizes both NashQ and Friend-and-Foe-Q: in general-sum games, the set of correlated equilibria contains the set of Nash equilibria; in constantsum games, the set of correlated equilibria contains the set of minimax equilibria. This paper describes experiments with four variants of CE-Q, demonstrating empirical convergence to equilibrium policies on a testbed of general-sum Markov games.},
	author = {Greenwald, Amy and Hall, Keith},
	file = {Greenwald and Hall - Correlated Q-Learning.pdf:files/369/Greenwald and Hall - Correlated Q-Learning.pdf:application/pdf},
	language = {en},
	pages = {8},
	title = {Correlated {Q}-{Learning}}}

@article{ruff_unifying_2021,
	abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
	annote = {Comment: 40 pages; accepted for publication in the Proceedings of the IEEE;},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Gr{\'e}goire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and M{\"u}ller, Klaus-Robert},
	doi = {10.1109/JPROC.2021.3052449},
	issn = {0018-9219, 1558-2256},
	journal = {Proceedings of the IEEE},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	note = {arXiv: 2009.11732},
	pages = {1--40},
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2009.11732},
	urldate = {2021-03-09},
	year = {2021},
	Bdsk-Url-1 = {http://arxiv.org/abs/2009.11732},
	Bdsk-Url-2 = {https://doi.org/10.1109/JPROC.2021.3052449}}

@article{ruff_unifying_2021-1,
	abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Gr{\'e}goire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and M{\"u}ller, Klaus-Robert},
	doi = {10.1109/JPROC.2021.3052449},
	issn = {0018-9219, 1558-2256},
	journal = {Proceedings of the IEEE},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	note = {arXiv: 2009.11732},
	pages = {1--40},
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2009.11732},
	urldate = {2021-03-09},
	year = {2021},
	Bdsk-Url-1 = {http://arxiv.org/abs/2009.11732},
	Bdsk-Url-2 = {https://doi.org/10.1109/JPROC.2021.3052449}}

@article{gomes_computational_2019,
	abstract = {Computer and information scientists join forces with other fields to help solve societal and environmental challenges facing humanity, in pursuit of a sustainable future.},
	author = {Gomes, Carla and Dietterich, Thomas and Barrett, Christopher and Conrad, Jon and Dilkina, Bistra and Ermon, Stefano and Fang, Fei and Farnsworth, Andrew and Fern, Alan and Fern, Xiaoli and Fink, Daniel and Fisher, Douglas and Flecker, Alexander and Freund, Daniel and Fuller, Angela and Gregoire, John and Hopcroft, John and Kelling, Steve and Kolter, Zico and Powell, Warren and Sintov, Nicole and Selker, John and Selman, Bart and Sheldon, Daniel and Shmoys, David and Tambe, Milind and Wong, Weng-Keen and Wood, Christopher and Wu, Xiaojian and Xue, Yexiang and Yadav, Amulya and Yakubu, Abdul-Aziz and Zeeman, Mary Lou},
	doi = {10.1145/3339399},
	file = {Gomes et al. - 2019 - Computational sustainability computing for a bett.pdf:files/384/Gomes et al. - 2019 - Computational sustainability computing for a bett.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	language = {en},
	month = aug,
	number = {9},
	pages = {56--65},
	shorttitle = {Computational sustainability},
	title = {Computational sustainability: computing for a better world and a sustainable future},
	url = {https://dl.acm.org/doi/10.1145/3339399},
	urldate = {2021-03-09},
	volume = {62},
	year = {2019},
	Bdsk-Url-1 = {https://dl.acm.org/doi/10.1145/3339399},
	Bdsk-Url-2 = {https://doi.org/10.1145/3339399}}

@article{gomes_computational_2019-1,
	abstract = {Computer and information scientists join forces with other fields to help solve societal and environmental challenges facing humanity, in pursuit of a sustainable future.},
	author = {Gomes, Carla and Dietterich, Thomas and Barrett, Christopher and Conrad, Jon and Dilkina, Bistra and Ermon, Stefano and Fang, Fei and Farnsworth, Andrew and Fern, Alan and Fern, Xiaoli and Fink, Daniel and Fisher, Douglas and Flecker, Alexander and Freund, Daniel and Fuller, Angela and Gregoire, John and Hopcroft, John and Kelling, Steve and Kolter, Zico and Powell, Warren and Sintov, Nicole and Selker, John and Selman, Bart and Sheldon, Daniel and Shmoys, David and Tambe, Milind and Wong, Weng-Keen and Wood, Christopher and Wu, Xiaojian and Xue, Yexiang and Yadav, Amulya and Yakubu, Abdul-Aziz and Zeeman, Mary Lou},
	doi = {10.1145/3339399},
	file = {Gomes et al. - 2019 - Computational sustainability computing for a bett.pdf:files/387/Gomes et al. - 2019 - Computational sustainability computing for a bett.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	language = {en},
	month = aug,
	number = {9},
	pages = {56--65},
	shorttitle = {Computational sustainability},
	title = {Computational sustainability: computing for a better world and a sustainable future},
	url = {https://dl.acm.org/doi/10.1145/3339399},
	urldate = {2021-03-09},
	volume = {62},
	year = {2019},
	Bdsk-Url-1 = {https://dl.acm.org/doi/10.1145/3339399},
	Bdsk-Url-2 = {https://doi.org/10.1145/3339399}}

@inproceedings{yessou_comparative_2020,
	abstract = {This paper analyzes and compares different deep learning loss functions in the framework of multi-label remote sensing (RS) image scene classification problems. We consider seven loss functions: 1) cross-entropy loss; 2) focal loss; 3) weighted cross-entropy loss; 4) Hamming loss; 5) Huber loss; 6) ranking loss; and 7) sparseMax loss. All the considered loss functions are analyzed for the first time in RS. After a theoretical analysis, an experimental analysis is carried out to compare the considered loss functions in terms of their: 1) overall accuracy; 2) class imbalance awareness (for which the number of samples associated to each class significantly varies); 3) convexibility and differentiability; and 4) learning efficiency (i.e., convergence speed). On the basis of our analysis, some guidelines are derived for a proper selection of a loss function in multi-label RS scene classification problems.},
	annote = {This paper examines the most common loss functions to use with deep learning and analyses their impact on training performance for a multi-label classification problem using satellite images. Good summary of methods and analysis of the tradeoffs.

Comment: Accepted at IEEE International Geoscience and Remote Sensing Symposium (IGARSS) 2020. For code visit: https://gitlab.tubit.tu-berlin.de/rsim/RS-MLC-Losses},
	author = {Yessou, Hichame and Sumbul, Gencer and Demir, Beg{\"u}m},
	booktitle = {IEEE International Geoscience and Remote Sensing Symposium (IGARSS).},
	date-modified = {2021-04-11 15:59:32 -0400},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, deep learning fundamentals, loss functions},
	month = sep,
	note = {arXiv: 2009.13935},
	title = {A {Comparative} {Study} of {Deep} {Learning} {Loss} {Functions} for {Multi}-{Label} {Remote} {Sensing} {Image} {Classification}},
	url = {http://arxiv.org/abs/2009.13935},
	urldate = {2021-04-11},
	year = {2020},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAVcGFwZXJzLzIwMDkuMTM5MzUucGRmTxEBvgAAAAABvgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////DjIwMDkuMTM5MzUucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQADAAAKIGN1AAAAAAAAAAAAAAAAAAZwYXBlcnMAAgBjLzpVc2VyczptY3Jvd2xleTpEb2N1bWVudHM6REtNQTpfYmlibGlvZ3JhcGh5OkRLTUFfem90ZXJvX2V4cG9ydF8yMDIxMDQxMS5iaWI6cGFwZXJzOjIwMDkuMTM5MzUucGRmAAAOAB4ADgAyADAAMAA5AC4AMQAzADkAMwA1AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBhVXNlcnMvbWNyb3dsZXkvRG9jdW1lbnRzL0RLTUEvX2JpYmxpb2dyYXBoeS9ES01BX3pvdGVyb19leHBvcnRfMjAyMTA0MTEuYmliL3BhcGVycy8yMDA5LjEzOTM1LnBkZgAAEwABLwAAFQACAA///wAAAAgADQAaACQAPAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAH+},
	Bdsk-Url-1 = {http://arxiv.org/abs/2009.13935}}
